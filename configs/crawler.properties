crawler.urlQueue.kafka.consumerMaxPoolRecords=512
crawler.urlQueue.kafka.consumerGroupId=1
crawler.urlQueue.kafka.topicName=urls
crawler.urlQueue.kafka.producerClientId=UrlQueueProducer
crawler.urlQueue.kafka.manualPartitionAssignment=false
crawler.urlQueue.kafka.partition=0
crawler.duplicate_checker.maxSize=1000
crawler.duplicate_checker.loadFromDatabase=false
crawler.duplicate_checker.putLinkBulkSize=200
crawler.scheduler.activeCrawlers=150
crawler.scheduler.queueSize=1024
crawler.pageFetcher.timeout=3000
crawler.pageFetcher.followRedirects=true
crawler.lru.initialCapacity=1500
crawler.initSeeds=true
crawler.lru.caffeine.expireSeconds=30
crawler.lru.caffeine.maximumSize=18000
crawler.persister.persisterThreadNumber=2
crawler.persister.pageQueueSize=300

crawler.persister.db.elastic.index=posts
crawler.persister.db.elastic.document=_doc
crawler.persister.db.elastic.flushNumberLimit=97
crawler.persister.db.elastic.flushSizeLimit=2

crawler.persister.db.hbase.flushNumberLimit=97
crawler.persister.db.hbase.crawledLink.tableName=crawledLinks
crawler.persister.db.hbase.crawledLink.columnFamily=partition
crawler.persister.db.hbase.crawledLink.qualifier=number

crawler.persister.db.hbase.pages.tableName=pages
crawler.persister.db.hbase.pages.columnFamily=data
crawler.persister.db.hbase.pages.qualifiers=link;context

crawler.persister.db.hbase.backLinks.tableName=backLinks
crawler.persister.db.hbase.backLinks.columnFamily=links

crawler.webserver.port=50001